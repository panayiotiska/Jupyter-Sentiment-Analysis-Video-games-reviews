{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>\n",
    "         <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/Introduction.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_previous.jpg\" width= 70% height= 70%>\n",
    "    </td><td>\n",
    "        <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/Index.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_table-of-contents.jpg\" width= 70% height= 70%>\n",
    "    </td><td>\n",
    "         <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/[Intro]Evaluation_metrics.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_next.jpg\" width= 70% height= 70%>\n",
    "    </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "Evaluation metrics play a critical role in achieving the optimal classifier during the classification training. Usually, the evaluation metrics are used to measure and summarize the quality of trained classifier when tested with the unseen data. Generally the evaluation metrics were employed as an evaluator for model selection. In this case, the evaluation metric task is to determine the best classifier among different types of trained classifiers which focus on the best future performance. In order to achieve this, sklearn's (library) metrics and scoring will be used and more specifically, the functions accuracy_score(), confusion_matrix() and classification_report().\n",
    "\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy is the quintessential classification metric, it is the number of correct predictions made by the model over all kinds predictions made. Represented below as an equation:\n",
    "\n",
    "<i>Correct predictions = True Positives + True Neutral + True Negatives\n",
    "\n",
    "All predictions = True Positives + False Positives + True Neutral + False Neutral + True Negatives + False Negatives</i>\n",
    "\n",
    "**Accuracy = (Correct predictions)/(All predictions)**\n",
    "\n",
    "Although, accuracy is not always enough to evaluate a model, especially in a dataset with not balanced classes.\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix, is a table with two dimensions (“True” and “Predicted”), and sets of “classes” in both dimensions. The confusion matrix printed above is shown below as a plot in order to be explained:<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/confusion_matrix_example.png\" width=\"350\" align=\"center\"/>\n",
    "</div>\n",
    "\n",
    "As illustrated above in the plot, the X-Axis represents the predicted labels, those predicted by the model while the Y-Axis shows the true label, the actual label of the data.\n",
    "\n",
    "The most important information in a confusion matrix is in the main diagonal which includes the values predicted correct by the model. Thus, in the main diagonal is where a higher amount is more pleasant (darker colour in the heat-map, often used for presenting confusion matrices).\n",
    "\n",
    "It is also important to examine for which classes the model is more sensitive to making mistakes. The false predicted labels are the values in the rest of the matrics, except the main diagonal. For example, in the confusion matrix above, the value \"812\" is the number of negative reviews which our model wrongly predicted as neutral, while 1373 in the number of reviews predicted as negative while they were originally neutral.\n",
    "\n",
    "## Classification Report\n",
    "\n",
    "The classification_report function builds a text report showing the main classification metrics. Those metrics are examined below:<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/classification_report_example.png\" width=\"400\" align=\"center\"/>\n",
    "</div><br><br>\n",
    "\n",
    "- ### Percision\n",
    "\n",
    "    To begin with an example, the percision of the positive class is the proportion of predicted Positives which is truly Positive. Represented below as an equation:\n",
    "\n",
    "    <i>Precision = (True Positives)/(True Positives + False Positives)\n",
    "\n",
    "    In our example which consists of three classes, the False Positives are those wrongly predicted as Negatives or Neutrals while they where originally Positive.\n",
    "    </i>\n",
    "\n",
    "    In the same way the percision for every class is calculated.\n",
    "\n",
    "- ### Recall\n",
    "\n",
    "    The recall of the positive class is the proportion of actual Positives which is correctly classified. Represented below as an equation:\n",
    "\n",
    "    <i>Recall = (True Positives)/(True Positives + False Negatives + False Neutral)\n",
    "\n",
    "    In our example which consists of three classes, the False Negatives are those wrongly predicted as Positives or Neutrals while they where originally Negative. And also the False Neutrals are those wrongly predicted as Positives or Negatives while they where originally Neutrals.\n",
    "    </i>\n",
    "    \n",
    "    In the same way the Recall for every class is calculated.\n",
    "\n",
    "- ### F1-Score\n",
    "\n",
    "    The F1 score is a number between 0 and 1 and is the harmonic mean of precision and recall.<br><br>\n",
    "\n",
    "    <div>\n",
    "    <img src=\"figures/f1_score.png\" width=\"230\" align=\"left\"/>\n",
    "    </div><br><br><br>\n",
    "    \n",
    "- ### Support \n",
    "    \n",
    "    The support is the number of occurrences of each class.\n",
    "    \n",
    "- ### Macro average:\n",
    "Calculates metrics for each label, and finds their unweighted mean. It does not take label imbalance into account.\n",
    "\n",
    "- ### Weighted average:\n",
    "Calculates metrics for each label, and finds their average weighted by support (the number of true instances for each label). It alters ‘macro’ to account for label imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/[Data_Exploration]Data_Lookup.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_next.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
