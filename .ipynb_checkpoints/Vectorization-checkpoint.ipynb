{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>\n",
    "         <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/[Data_Exploration]Word-Clouds.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_previous.jpg\" width= 70% height= 70%>\n",
    "    </td><td>\n",
    "        <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/Index.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_table-of-contents.jpg\" width= 70% height= 70%>\n",
    "    </td><td>\n",
    "         <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/Naive_Bayes_&_SVM_tfidfVectorizer.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_next.jpg\" width= 70% height= 70%>\n",
    "    </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Methods\n",
    "\n",
    "It is commonly known that machines are not capable enough of understanding words or sentences in the same manner as humans do.\n",
    "In order to make documents corpora more palatable for computers, they must first be converted into a numerical structure or representation. For classification, requirements are represented as vectors in a multidimensional space regardless of the vectorization method. Each dimension of a vector has a weight reflecting characteristics of the requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "The tfâ€“idf is the product of two statistics, term frequency and inverse document frequency.\n",
    "\n",
    "#### Term Frequency (TF)\n",
    "\n",
    "Term frequency is a weight representing how often a word occurs in a document or in this case a review.\n",
    "\n",
    "\\begin{equation*}\n",
    "tf_{i,j} = \\frac{n_{i,j}}{\\sum_{k}n_{i,j}}\n",
    "\\end{equation*}\n",
    "\n",
    "At the equation above, the left side represents the term frequency of the term *i* in the review *j* and it equals, the number of times the token *n* is being found in the examined review, devided by the total number of words in the document\n",
    "\n",
    "#### Inverse Data Frequency (IDF)\n",
    "\n",
    "Inverse data frequency is a weight representing how common a token is across all the reviews. The IDF value decreases for the more reviews the token appears on.\n",
    "\n",
    "\\begin{equation*}\n",
    "idf(w) = log(\\frac{N}{df_{t}})\n",
    "\\end{equation*}\n",
    "\n",
    "The IDF metric of the token *w* is calculated by the logarithm of the the total number of documents, devided by the number of documents the token appears on.\n",
    "\n",
    "#### Calculate TF-IDF\n",
    "\n",
    "\\begin{equation*}\n",
    "tfidf_{i,j}(w) = tf_{i,j} . idf(w)\n",
    "\\end{equation*}\n",
    "\n",
    "TF-IDF is simply calculated by multiplying the the two metrics already calculated above.\n",
    "\n",
    "In the following algorithm, an implementation of TF-IDF is applied using the sklearn library. In this implementation the value of one is being added in both the numerator and denominator in order to prevent returning the value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer\n",
    "\n",
    "Instead of storing the tokens as strings, the vectorizer applies the hashing trick to encode them as numerical indexes. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array.\n",
    "\n",
    "The hashing trick leads to large memory savings, first of all, because it can operate directly on the input term strings and avoids the use of a dictionary to translate words into vectors, also, the parameter vector of a learning model lives within the much smaller dimensional space. The dimensionality reduction comes at the cost of collisions, where multiple words are mapped into the same dimension.\n",
    "\n",
    "Both HashingVectorizer and CountVectorizer convert a collection of text documents to a matrix of token occurrences. The difference is that HashingVectorizer does not store the resulting vocabulary as already mentioned, that means that it is easier to include all tokens without the need of using the max_features parameter of the CountVectorizer.\n",
    "\n",
    "The main idea is easier to understand through a simple CountVectorizer example of the following reviews:\n",
    "- This is a review\n",
    "- Hate this videogame\n",
    "\n",
    "<img src=\"figures/countvect.png\" width=\"450\" align=\"center\"/>\n",
    "\n",
    "#### The hashing vectorizer algorithms:\n",
    "\n",
    "- Finding the hash value of each word (the hash function employed is the signed 32-bit version of Murmurhash3) and calculate the modulo of the hash by the size *n* of the output array. \n",
    "\n",
    "<img src=\"figures/murmur.png\" width=\"750\" align=\"center\"/>\n",
    "\n",
    "- Save each feature at the array position it corresponds based on its modulo value.\n",
    "\n",
    "<img src=\"figures/hashindex.png\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "Every array record is a linked list pointing to the next feature which happened to have the same modulo value as shown below.\n",
    "\n",
    "<img src=\"figures/linkedlist.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "The final form of the data structure extracted by the hashing vectorizer is shown below, with each column being a linked list.\n",
    "\n",
    "<img src=\"figures/finalhash.png\" width=\"500\" align=\"center\"/>\n",
    "<br /><br />\n",
    "The binary HashingVectorizer provided by sklearn by using the parameter *binary=True* is used for discrete probabilistic models that model binary events rather than integer counts. It is most commonly used for natular language processing problems as it takes into account if a word is included in a document/review but not the number of times it occured and this may be more valuable for small documents like reviews.\n",
    "\n",
    "In the next three notebooks the following vectorization algorithms will be tested: TfidfVectorizer, HashingVectorizer and HashingVectorizer with binary parameter set to *True*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/Naive_Bayes_&_SVM_tfidfVectorizer.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_next.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
