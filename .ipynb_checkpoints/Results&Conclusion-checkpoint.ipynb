{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>\n",
    "         <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/SVM_HashingVectorizer_Regex.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_previous.jpg\" width= 70% height= 70%>\n",
    "    </td><td>\n",
    "        <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/Index.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_table-of-contents.jpg\" width= 70% height= 70%>\n",
    "    </td><td>\n",
    "         <a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/References.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_next.jpg\" width= 70% height= 70%>\n",
    "    </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Conclusion\n",
    "In this notebook, every sentiment analysis chapter is concluded into a graph including the metrics of its main models. The chapters included are: Vectorization, Stemming, Balance classes, Part of speech anaylsis and Objective Analysis based on the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization comparison\n",
    "First of all, the vectorization methods being tested were the TF-IDF vectorizer and the hashing vectorizer. As shown in the graph below, both vectorizers performed really good for the SVM (Support Vector Machines) classifier while on the other hand NB (Naive Bayes) classifier did not perform as expected. The hashing vectorizer for SVM performed slightly better that the TF-IDF with the binary hashing vectorizer resulting the best classification metrics out of all reaching 0.827 accuracy and keeping the rest of the values, including recall, f1-score and precision at more than 0.79. All in all, the model with the binary hashing vectorizer is kept for later analysis as it performed favorable.\n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/results_vectorization_comparison.png\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming comparison\n",
    "In the second chapter of the project, two stemming methods are tested for both classifiers, the Lancaster stemmer and the Snowball stemmer. For Naive Bayes the output metrics are significantly low. Using the SVM classifier, both stemmers permormed the same reaching the remarkable accuracy score of 0.82, although, using stemming did not result in overcoming the previously best metrics of 0.827 accuracy, thus, the procedure continues without using a stemming method.\n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/results_stemming_comparison.png\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class balancing comparison\n",
    "As mentioned in previous notebooks, the dataset's classes were not balanced, the TomekLinks under-sampling method was tested in order to balance the frequency of the samples for all the classes. As illustrated bellow balancing the classes using under-sampling did not help to increase the values of the classification metrics. Eventhough the TomekLinks technique with the majority parameter activated resulted to marginally better results than the initial TomekLinks function, the model with only vectorization is still on the top of the ladder.\n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/results_balance_comparison.png\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of speech filtering comparison\n",
    "An interesting approach was to try permorming sentiment analysis with only specific parts of speech, although, this technique did not result in any noteworthy scores. As presented below, the f1-score is dropping significantly when filtering out some parts of speech. More specifically, for the model with only adjectives and for the model with adjectives and adverbs the f1-score have dropped below 0.725 and the accuracy is around 0.78. On the other hand, the model with adjectives and nouns can be considered fair, because filtering out words from the corpora remarkably drops the running time, ending up with a usefull model for limited time situations.\n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/results_pos_comparison.png\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective analysis comparison - [Final model](https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/SVM_HashingVectorizer_Regex.ipynb)\n",
    "In the final sentiment-analysis chapter of the project different issues discused in the 'Data exploration' chapter are suppressed by, using regular expressions, adding custom stop-words, handling with emojis and annotations, dealing with the word \"not\" as well as removing undesirable text. In the graph below, a side to side comparison is illustrated, showing how this considerations resulted in better scores than the \"conquering\" model from all the previous chapters. Specifically, the final model's accuracy score is 0.83 and both the precision and f1-score have reached the score of 0.8.\n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/results_regex_comparison.png\" align=\"center\"/>\n",
    "</div>\n",
    "\n",
    "In the confusion matrix of the final model by looking at the main diagonal, it is important to mention that the values of the True Negatives and True Positives are significantly higher that their false values. More specifically, for the Positive class, the model correctly classified 51153 positive reviews while it wrongly classified only 1387, considered 818 of them negative and 569 of them neutral. For the Negative class, the model correctly classified 5055 negative reviews while it wrongly classified 3384, considered 764 of them neutral and 2620 of them positive. The neutral class seems to be the \"weaker\" class while most of the neutral reviews have been classified into the Positive class.\n",
    "<br><br>\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/final_model_confusion_matrix.png\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Overall, the model with the best accuracy metrics is the \"objective analysis\" model, which took into consideration the data exploration analysis. Going through all this different classification methods and processing techniques, many observations were made. First of all, starting from the fact that the dataset only included 231780 records/reviews, the information to be fitted into a classifier was limited. This concludes to the incident that every technique which would discard information by removing tokens from the corpora would not result to any better evaluation metrics for the current dataset. Another conclusion made, is that synthetic samples do not acquire a very accurate representation of the original samples as the tests made for balancing the classes did not emerge any improvement in the results. This may be affected by the huge gap there is between the samples of the classes of the specific dataset used. The most valuable consideration extracted out of this project is how important the data exploration is, in order to investigate the data working on and get important findings which can help discover the best pre-processing methods applicable for a particular dataset. All in all, it is important to use a different data processing apprach for every unique dataset. For this specific dataset, it was important to note exclusive stop-words based on the category the reviews belong to. Another example is the manipulation made on the emojis, which in a more formal corpora would not be an appropriate modification. Finally, by gathering all the extracted information, it is remarkable to mention that the accuracy metrics occured from the final model were significantly high for the present dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code sample for the graphs\n",
    "\n",
    "### Comparison graphs from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data\n",
    "data1 = pd.read_csv(r\"reports\\SVM_report_HashingVect.csv\").set_index('Unnamed: 0')\n",
    "data2 = pd.read_csv(r\"reports\\SVM_report_HashingVect_Regex.csv\").set_index('Unnamed: 0')\n",
    "\n",
    "# Set width of bar\n",
    "barWidth = 0.15\n",
    " \n",
    "# Set height of bar\n",
    "bars1 = [data1['precision']['weighted avg'], data2['precision']['weighted avg']]\n",
    "bars2 = [data1['recall']['weighted avg'], data2['recall']['weighted avg']]\n",
    "bars3 = [data1['f1-score']['weighted avg'], data2['f1-score']['weighted avg']]\n",
    "bars4 = [data1['recall']['accuracy'], data2['recall']['accuracy']]\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    "r4 = [x + barWidth for x in r3]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='Precision')\n",
    "plt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='Recall')\n",
    "plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='F1-Score')\n",
    "plt.bar(r4, bars4, color='#D2841C', width=barWidth, edgecolor='white', label='Accuracy')\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Results comparison', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], ['SVM Hashing', 'SVM Regex'])\n",
    " \n",
    "# Create legend & Show graphic\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(11,8)\n",
    "plt.legend()\n",
    "plt.ylim(0.74,0.85)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://nbviewer.jupyter.org/github/panayiotiska/Jupyter-Sentiment-Analysis-Video-games-reviews/blob/master/References.ipynb\">\n",
    "         <img alt=\"start\" src=\"figures/button_next.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
